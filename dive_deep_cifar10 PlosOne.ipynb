{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dive Deep into Training with CIFAR10\n",
    "==============================================\n",
    "\n",
    "Hope you enjoyed playing with our demo script.\n",
    "Now, you may be wandering: how exactly was the model trained?\n",
    "In this tutorial, we will focus on answering this question.\n",
    "\n",
    "Prerequisites\n",
    "-------------\n",
    "\n",
    "We assume readers have a basic understanding of ``Gluon``.\n",
    "If not, we suggest you spend 60 minutes to get started with the `Gluon Crash\n",
    "Course <http://gluon-crash-course.mxnet.io/index.html>`__ .\n",
    "\n",
    "As we all know, training deep neural networks on GPUs is way faster than\n",
    "training on CPU.\n",
    "In the previous tutorials, we used CPU because classifying a single image is a\n",
    "relatively easy task.\n",
    "However, since we are about to train a model, it is strongly recommended to\n",
    "use a machine with GPU(s).\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The rest of the tutorial walks you through the details of ``CIFAR10`` training.\n",
    "    If you want a quick start without knowing the details, try downloading\n",
    "    this script and start training with just one command.\n",
    "\n",
    "    :download:`Download train_cifar10.py<../../../scripts/classification/cifar/train_cifar10.py>`\n",
    "\n",
    "    Here's a sample command with recommended parameters:\n",
    "\n",
    "    ::\n",
    "\n",
    "        python train_cifar10.py --num-epochs 240 --mode hybrid --num-gpus 1 -j 8 --batch-size 128            --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 80,160 --model cifar_resnet20_v1</p></div>\n",
    "\n",
    "\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "from gluoncv.data import transforms as gcv_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARResNetV1_article(\n",
      "  (features): HybridSequential(\n",
      "    (0): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "    (2): HybridSequential(\n",
      "      (0): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (2): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (3): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (4): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (5): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (6): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (7): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (8): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): HybridSequential(\n",
      "      (0): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "          (1): Conv2D(16 -> 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (2): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (3): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (4): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (5): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (6): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (7): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (8): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): HybridSequential(\n",
      "      (0): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "          (1): Conv2D(32 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (2): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (3): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (4): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (5): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (6): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (7): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (8): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (output): Dense(64 -> 10, linear)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "import cifarresnet_article\n",
    "\n",
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model CIFAR_ResNet20_v1, with 10 output classes, without pre-trained weights\n",
    "net =cifarresnet_article.get_cifar_resnet(3, 56, classes=10)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous structures for convolutional neural networks.\n",
    "Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the\n",
    "tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARResNetV1_article(\n",
      "  (features): HybridSequential(\n",
      "    (0): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "    (2): HybridSequential(\n",
      "      (0): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (2): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (3): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (4): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (5): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (6): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (7): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (8): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): HybridSequential(\n",
      "      (0): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(16 -> 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "          (1): Conv2D(16 -> 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (2): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (3): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (4): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (5): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (6): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (7): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (8): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): HybridSequential(\n",
      "      (0): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(32 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "        (downsample): HybridSequential(\n",
      "          (0): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "          (1): Conv2D(32 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (2): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (3): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (4): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (5): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (6): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (7): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (8): CIFARBasicBlockV1_article(\n",
      "        (body): HybridSequential(\n",
      "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "          (2): Activation(relu)\n",
      "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  )\n",
      "  (output): Dense(64 -> 10, linear)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Data Loader\n",
    "---------------------------------\n",
    "\n",
    "Data augmentation is a common technique used for training. It is\n",
    "base on the assumption that, for the same object, photos under different\n",
    "composition, lighting condition, or color should all yield the same prediction.\n",
    "\n",
    "Here are photos of the Golden Bridge, taken by many people,\n",
    "at different time from different angles.\n",
    "We can easily tell that they are photos of the same thing.\n",
    "\n",
    "|image-golden-bridge|\n",
    "\n",
    "We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "input image. Our augmentation transforms the image with\n",
    "resizing, cropping, flipping and other techniques.\n",
    "\n",
    "With ``Gluon``, we can create our transform function as following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    transform_train = transforms.Compose([\n",
    "        gcv_transforms.RandomCrop(32, pad=4),\n",
    "        transforms.RandomFlipLeftRight(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that most of the operations are randomized. This in effect\n",
    "increases the number of different images the model sees during training.\n",
    "The more data we have, the better our model generalizes over\n",
    "unseen images.\n",
    "\n",
    "On the other hand, when making prediction, we would like to remove all\n",
    "random operations in order to get a deterministic result. The transform\n",
    "function for prediction is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is important to keep the normalization step, since the\n",
    "model only works well on inputs from the same distribution.\n",
    "\n",
    "With the transform functions, we can define data loaders for our\n",
    "training and validation datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss and Metric\n",
    "--------------------------\n",
    "\n",
    "Optimizer improves the model during training. Here we use the popular\n",
    "Nesterov accelerated gradient descent algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays32k 48k 64k\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent\n",
    "optimizer = 'nag'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "used in ``trainer``. One important idea in model training is to\n",
    "gradually decrease learning rate. This means the optimizer takes large\n",
    "steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "\n",
    "Our plan sets the learning rate to 0.1 at the beginning, then\n",
    "divide it by 10 at the 80-th epoch, then again at the 160-th epoch.\n",
    "We'll use `lr_decay_epoch` in the main training loop for this purpose.\n",
    "\n",
    "In order to optimize our model, we need a loss function.\n",
    "In essence, loss functions compute the difference between predictions and the\n",
    "ground-truth as a measure of model performance.\n",
    "We can then take the gradients of the loss w.r.t. the weights.\n",
    "Gradients points the optimizer to the direction weights should move to\n",
    "improve model performance.\n",
    "\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are similar to loss functions, but they are different in the\n",
    "following aspects:\n",
    "\n",
    "-  Metric is how we evaluate model performance. Each metric is related to a\n",
    "   specific task, but independent from the model training process.\n",
    "-  For classification, we usually only use one loss function to train\n",
    "   our model, but we can have several metrics for evaluating\n",
    "   performance.\n",
    "-  Loss function can be used as a metric, but sometimes its values are hard\n",
    "   to interpretate. For instance, the concept \"accuracy\" is\n",
    "   easier to understand than \"softmax cross entropy\"\n",
    "\n",
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation\n",
    "----------\n",
    "\n",
    "Validation dataset provides us a way of monitoring the training process.\n",
    "We have labels for validation data, but they are held out during training.\n",
    "Instead, we use them to evaluate the models performance on unseen data\n",
    "and prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate performance, we need a metric. Then, we loop\n",
    "through the validation data and predict with our model.\n",
    "We'll run this function at the end of every epoch to show improvement.\n",
    "over the last epoch.\n",
    "\n",
    "Training\n",
    "--------\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 3 epochs.\n",
    "  In your experiments, we recommend setting ``epochs=240``.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.289583 val=0.402700 loss=99056.122772 time: 56.970044\n",
      "[Epoch 1] train=0.435917 val=0.489400 loss=76409.246170 time: 55.297745\n",
      "[Epoch 2] train=0.538081 val=0.518900 loss=63710.887009 time: 57.474999\n",
      "[Epoch 3] train=0.619812 val=0.648500 loss=53603.132912 time: 55.240001\n",
      "[Epoch 4] train=0.675841 val=0.653700 loss=46023.463440 time: 55.156000\n",
      "[Epoch 5] train=0.721855 val=0.724400 loss=39610.167397 time: 55.271997\n",
      "[Epoch 6] train=0.754788 val=0.746700 loss=35115.769806 time: 56.638000\n",
      "[Epoch 7] train=0.778405 val=0.772300 loss=31832.857052 time: 55.071996\n",
      "[Epoch 8] train=0.799459 val=0.738500 loss=28839.897491 time: 54.875997\n",
      "[Epoch 9] train=0.812360 val=0.796500 loss=26958.478027 time: 53.376999\n",
      "[Epoch 10] train=0.826302 val=0.821900 loss=25011.168758 time: 53.362997\n",
      "[Epoch 11] train=0.838442 val=0.813300 loss=23394.630898 time: 54.654043\n",
      "[Epoch 12] train=0.845913 val=0.829800 loss=22282.579571 time: 58.496998\n",
      "[Epoch 13] train=0.856791 val=0.833400 loss=20931.197191 time: 57.270043\n",
      "[Epoch 14] train=0.862039 val=0.835500 loss=19919.713411 time: 60.549469\n",
      "[Epoch 15] train=0.865665 val=0.818900 loss=19346.070133 time: 63.016553\n",
      "[Epoch 16] train=0.873858 val=0.831900 loss=18504.957630 time: 61.055656\n",
      "[Epoch 17] train=0.877344 val=0.837000 loss=17685.570929 time: 61.928899\n",
      "[Epoch 18] train=0.880829 val=0.822900 loss=17169.289801 time: 61.870179\n",
      "[Epoch 19] train=0.884215 val=0.840100 loss=16681.922825 time: 60.437048\n",
      "[Epoch 20] train=0.887260 val=0.829600 loss=16209.858679 time: 59.270997\n",
      "[Epoch 21] train=0.891687 val=0.839200 loss=15730.840359 time: 59.607556\n",
      "[Epoch 22] train=0.892909 val=0.824600 loss=15351.859272 time: 59.735790\n",
      "[Epoch 23] train=0.895553 val=0.852400 loss=15077.567993 time: 59.672088\n",
      "[Epoch 24] train=0.900541 val=0.867900 loss=14365.912102 time: 58.630055\n",
      "[Epoch 25] train=0.901082 val=0.853300 loss=14319.917397 time: 59.027038\n",
      "[Epoch 26] train=0.904026 val=0.850400 loss=13759.249938 time: 60.054000\n",
      "[Epoch 27] train=0.905809 val=0.846500 loss=13577.727514 time: 60.191571\n",
      "[Epoch 28] train=0.906651 val=0.850400 loss=13408.739338 time: 56.856490\n",
      "[Epoch 29] train=0.907212 val=0.874200 loss=13204.738192 time: 61.812571\n",
      "[Epoch 30] train=0.907853 val=0.856300 loss=13047.311482 time: 56.228995\n",
      "[Epoch 31] train=0.911178 val=0.858900 loss=12627.502230 time: 61.381037\n",
      "[Epoch 32] train=0.914864 val=0.864400 loss=12186.145184 time: 57.401449\n",
      "[Epoch 33] train=0.917087 val=0.850900 loss=11850.900454 time: 56.887211\n",
      "[Epoch 34] train=0.915966 val=0.849800 loss=11986.364646 time: 56.004019\n",
      "[Epoch 35] train=0.917368 val=0.867900 loss=11658.254181 time: 61.150000\n",
      "[Epoch 36] train=0.918450 val=0.860800 loss=11626.478085 time: 56.694000\n",
      "[Epoch 37] train=0.922316 val=0.838700 loss=11215.896016 time: 56.714998\n",
      "[Epoch 38] train=0.921194 val=0.854400 loss=11290.519439 time: 56.424997\n",
      "[Epoch 39] train=0.921034 val=0.850000 loss=11322.452468 time: 55.812601\n",
      "[Epoch 40] train=0.922736 val=0.859900 loss=10936.901279 time: 56.541584\n",
      "[Epoch 41] train=0.923297 val=0.861000 loss=10901.700027 time: 58.257999\n",
      "[Epoch 42] train=0.923718 val=0.844200 loss=10866.235335 time: 56.653999\n",
      "[Epoch 43] train=0.927424 val=0.860400 loss=10400.181056 time: 55.702999\n",
      "[Epoch 44] train=0.924519 val=0.872900 loss=10589.451403 time: 59.086000\n",
      "[Epoch 45] train=0.925280 val=0.874400 loss=10517.549274 time: 57.006998\n",
      "[Epoch 46] train=0.928005 val=0.871700 loss=10319.311943 time: 56.097999\n",
      "[Epoch 47] train=0.929367 val=0.860200 loss=10054.649313 time: 58.870042\n",
      "[Epoch 48] train=0.930088 val=0.872100 loss=9946.098482 time: 59.920802\n",
      "[Epoch 49] train=0.929808 val=0.865700 loss=10021.464468 time: 61.026645\n",
      "[Epoch 50] train=0.930789 val=0.855700 loss=9928.061542 time: 60.996675\n",
      "[Epoch 51] train=0.933474 val=0.880000 loss=9630.119248 time: 61.452013\n",
      "[Epoch 52] train=0.932091 val=0.877400 loss=9677.240819 time: 66.912321\n",
      "[Epoch 53] train=0.934075 val=0.858500 loss=9432.292251 time: 57.778517\n",
      "[Epoch 54] train=0.931230 val=0.862400 loss=9668.461359 time: 55.976999\n",
      "[Epoch 55] train=0.932532 val=0.894200 loss=9589.227557 time: 54.971570\n",
      "[Epoch 56] train=0.934475 val=0.876600 loss=9473.358927 time: 54.914999\n",
      "[Epoch 57] train=0.934115 val=0.879000 loss=9411.909680 time: 57.078999\n",
      "[Epoch 58] train=0.935757 val=0.875200 loss=9147.543556 time: 57.019999\n",
      "[Epoch 59] train=0.937360 val=0.873200 loss=8958.279284 time: 54.895997\n",
      "[Epoch 60] train=0.937019 val=0.869300 loss=8990.013728 time: 54.792735\n",
      "[Epoch 61] train=0.936839 val=0.880500 loss=8993.606996 time: 54.451526\n",
      "[Epoch 62] train=0.937019 val=0.855400 loss=8974.779546 time: 59.246748\n",
      "[Epoch 63] train=0.938381 val=0.877100 loss=8799.992336 time: 60.022141\n",
      "[Epoch 64] train=0.936178 val=0.863100 loss=8991.530979 time: 56.483368\n",
      "[Epoch 65] train=0.938341 val=0.873500 loss=8821.408415 time: 56.615039\n",
      "[Epoch 66] train=0.937460 val=0.879200 loss=8891.858235 time: 56.870734\n",
      "[Epoch 67] train=0.939163 val=0.861600 loss=8756.671091 time: 58.915040\n",
      "[Epoch 68] train=0.939063 val=0.845100 loss=8701.523674 time: 57.878096\n",
      "[Epoch 69] train=0.941026 val=0.876900 loss=8493.799935 time: 55.569016\n",
      "[Epoch 70] train=0.938682 val=0.868500 loss=8677.008287 time: 55.259987\n",
      "[Epoch 71] train=0.940084 val=0.867500 loss=8538.775173 time: 55.175018\n",
      "[Epoch 72] train=0.940244 val=0.875900 loss=8439.976102 time: 55.457000\n",
      "[Epoch 73] train=0.940825 val=0.865000 loss=8525.417165 time: 55.375998\n",
      "[Epoch 74] train=0.942448 val=0.883700 loss=8219.370023 time: 55.319000\n",
      "[Epoch 75] train=0.940925 val=0.858200 loss=8400.540280 time: 59.205194\n",
      "[Epoch 76] train=0.941026 val=0.850300 loss=8447.020533 time: 57.358557\n",
      "[Epoch 77] train=0.942388 val=0.888500 loss=8199.021278 time: 58.171765\n",
      "[Epoch 78] train=0.942348 val=0.879100 loss=8296.431764 time: 54.332735\n",
      "[Epoch 79] train=0.943029 val=0.890000 loss=8196.249848 time: 54.212997\n",
      "[Epoch 80] train=0.972596 val=0.920300 loss=4098.659753 time: 54.298648\n",
      "[Epoch 81] train=0.981430 val=0.921900 loss=2885.529096 time: 54.040999\n",
      "[Epoch 82] train=0.985497 val=0.923500 loss=2271.367613 time: 54.257000\n",
      "[Epoch 83] train=0.986398 val=0.924500 loss=2103.051835 time: 53.941000\n",
      "[Epoch 84] train=0.988542 val=0.926100 loss=1852.741972 time: 54.019001\n",
      "[Epoch 85] train=0.989463 val=0.927200 loss=1665.411414 time: 54.388998\n",
      "[Epoch 86] train=0.990986 val=0.926400 loss=1516.873790 time: 54.241000\n",
      "[Epoch 87] train=0.991226 val=0.924000 loss=1371.888239 time: 53.863000\n",
      "[Epoch 88] train=0.991947 val=0.926800 loss=1286.167051 time: 53.846318\n",
      "[Epoch 89] train=0.992849 val=0.926900 loss=1164.626604 time: 54.141999\n",
      "[Epoch 90] train=0.992288 val=0.925000 loss=1175.281240 time: 54.219998\n",
      "[Epoch 91] train=0.993229 val=0.924600 loss=1063.091062 time: 57.589870\n",
      "[Epoch 92] train=0.993670 val=0.923900 loss=992.080120 time: 57.932034\n",
      "[Epoch 93] train=0.994151 val=0.924900 loss=932.804871 time: 55.987998\n",
      "[Epoch 94] train=0.994651 val=0.924600 loss=929.581678 time: 55.104132\n",
      "[Epoch 95] train=0.994792 val=0.925200 loss=821.122471 time: 54.527267\n",
      "[Epoch 96] train=0.995192 val=0.924800 loss=809.543790 time: 54.629188\n",
      "[Epoch 97] train=0.995292 val=0.927200 loss=810.475572 time: 54.721953\n",
      "[Epoch 98] train=0.996194 val=0.924800 loss=681.514485 time: 54.305623\n",
      "[Epoch 99] train=0.996274 val=0.926800 loss=668.946564 time: 54.469880\n",
      "[Epoch 100] train=0.995373 val=0.924900 loss=725.716890 time: 55.475595\n",
      "[Epoch 101] train=0.996935 val=0.925200 loss=579.905526 time: 54.681124\n",
      "[Epoch 102] train=0.996554 val=0.925300 loss=596.370552 time: 54.671305\n",
      "[Epoch 103] train=0.996454 val=0.923400 loss=609.408797 time: 54.819120\n",
      "[Epoch 104] train=0.996394 val=0.922900 loss=605.563175 time: 54.608769\n",
      "[Epoch 105] train=0.996915 val=0.925100 loss=551.034192 time: 55.223239\n",
      "[Epoch 106] train=0.996575 val=0.925100 loss=579.245162 time: 55.926426\n",
      "[Epoch 107] train=0.997256 val=0.925800 loss=497.446665 time: 54.704847\n",
      "[Epoch 108] train=0.997316 val=0.926300 loss=469.448268 time: 54.759074\n",
      "[Epoch 109] train=0.997316 val=0.925100 loss=469.560407 time: 54.679419\n",
      "[Epoch 110] train=0.996695 val=0.924800 loss=563.526867 time: 54.852050\n",
      "[Epoch 111] train=0.997576 val=0.925000 loss=442.525184 time: 55.587160\n",
      "[Epoch 112] train=0.997776 val=0.926200 loss=409.402532 time: 54.770693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 113] train=0.997676 val=0.925200 loss=443.799749 time: 55.475575\n",
      "[Epoch 114] train=0.997656 val=0.925000 loss=434.429669 time: 54.639586\n",
      "[Epoch 115] train=0.998017 val=0.926300 loss=398.578651 time: 54.256085\n",
      "[Epoch 116] train=0.997576 val=0.924900 loss=425.232158 time: 54.180727\n",
      "[Epoch 117] train=0.997676 val=0.926400 loss=424.003524 time: 54.762000\n",
      "[Epoch 118] train=0.997396 val=0.925900 loss=440.613508 time: 54.026000\n",
      "[Epoch 119] train=0.997937 val=0.924000 loss=373.639119 time: 56.727036\n",
      "[Epoch 120] train=0.997716 val=0.925600 loss=401.916674 time: 56.399147\n",
      "[Epoch 121] train=0.997796 val=0.925600 loss=390.370795 time: 55.263178\n",
      "[Epoch 122] train=0.997817 val=0.927500 loss=396.607212 time: 54.941559\n",
      "[Epoch 123] train=0.997897 val=0.921500 loss=368.264090 time: 56.220516\n",
      "[Epoch 124] train=0.998137 val=0.926900 loss=359.614473 time: 55.828085\n",
      "[Epoch 125] train=0.997656 val=0.926800 loss=373.756313 time: 55.603303\n",
      "[Epoch 126] train=0.998197 val=0.927000 loss=326.873049 time: 56.079754\n",
      "[Epoch 127] train=0.998257 val=0.924500 loss=355.711023 time: 57.267524\n",
      "[Epoch 128] train=0.997977 val=0.923400 loss=357.049529 time: 57.705000\n",
      "[Epoch 129] train=0.997837 val=0.927000 loss=371.831967 time: 58.154366\n",
      "[Epoch 130] train=0.998097 val=0.925200 loss=358.600521 time: 59.601038\n",
      "[Epoch 131] train=0.998277 val=0.924500 loss=346.659309 time: 58.231783\n",
      "[Epoch 132] train=0.998017 val=0.925100 loss=342.954294 time: 57.433483\n",
      "[Epoch 133] train=0.998437 val=0.926100 loss=300.916021 time: 57.598968\n",
      "[Epoch 134] train=0.998277 val=0.925200 loss=330.635145 time: 56.746802\n",
      "[Epoch 135] train=0.998157 val=0.925700 loss=318.981337 time: 56.223998\n",
      "[Epoch 136] train=0.998157 val=0.924200 loss=334.209884 time: 57.166166\n",
      "[Epoch 137] train=0.998638 val=0.926200 loss=275.935335 time: 56.382921\n",
      "[Epoch 138] train=0.998397 val=0.925900 loss=300.720204 time: 56.429741\n",
      "[Epoch 139] train=0.998097 val=0.925600 loss=324.684775 time: 55.783724\n",
      "[Epoch 140] train=0.998898 val=0.924800 loss=231.953070 time: 56.390806\n",
      "[Epoch 141] train=0.997796 val=0.923700 loss=342.239890 time: 55.120192\n",
      "[Epoch 142] train=0.997917 val=0.924700 loss=339.629570 time: 55.185192\n",
      "[Epoch 143] train=0.998237 val=0.925700 loss=324.025580 time: 55.973102\n",
      "[Epoch 144] train=0.998177 val=0.925200 loss=306.417943 time: 55.629921\n",
      "[Epoch 145] train=0.997716 val=0.924800 loss=367.576197 time: 55.634059\n",
      "[Epoch 146] train=0.998137 val=0.924900 loss=332.955817 time: 55.235850\n",
      "[Epoch 147] train=0.997496 val=0.925900 loss=389.270904 time: 56.977419\n",
      "[Epoch 148] train=0.998097 val=0.925100 loss=328.731153 time: 54.464795\n",
      "[Epoch 149] train=0.998117 val=0.926000 loss=338.231145 time: 55.189682\n",
      "[Epoch 150] train=0.998037 val=0.925700 loss=319.682335 time: 55.043278\n",
      "[Epoch 151] train=0.997977 val=0.925800 loss=330.726819 time: 55.221228\n",
      "[Epoch 152] train=0.998217 val=0.925900 loss=309.334333 time: 56.000139\n",
      "[Epoch 153] train=0.998357 val=0.924900 loss=297.807618 time: 55.822386\n",
      "[Epoch 154] train=0.998277 val=0.925800 loss=303.193834 time: 55.879187\n",
      "[Epoch 155] train=0.998598 val=0.925300 loss=278.741882 time: 57.000032\n",
      "[Epoch 156] train=0.998437 val=0.924400 loss=275.082102 time: 55.529743\n",
      "[Epoch 157] train=0.998297 val=0.925700 loss=313.308480 time: 55.753701\n",
      "[Epoch 158] train=0.998698 val=0.925400 loss=255.521215 time: 55.933620\n",
      "[Epoch 159] train=0.998317 val=0.924900 loss=296.763452 time: 55.966114\n",
      "[Epoch 160] train=0.998838 val=0.927000 loss=222.019576 time: 56.353792\n",
      "[Epoch 161] train=0.998898 val=0.928400 loss=205.917668 time: 55.842000\n",
      "[Epoch 162] train=0.999018 val=0.927300 loss=195.425636 time: 58.467233\n",
      "[Epoch 163] train=0.999219 val=0.927100 loss=175.788536 time: 58.572398\n",
      "[Epoch 164] train=0.999259 val=0.929200 loss=166.989002 time: 57.482642\n",
      "[Epoch 165] train=0.999279 val=0.929000 loss=161.244548 time: 56.657233\n",
      "[Epoch 166] train=0.999379 val=0.928800 loss=156.051919 time: 56.313945\n",
      "[Epoch 167] train=0.999279 val=0.929600 loss=154.372546 time: 56.060887\n",
      "[Epoch 168] train=0.999239 val=0.929300 loss=167.855085 time: 56.270991\n",
      "[Epoch 169] train=0.999359 val=0.929500 loss=142.493153 time: 55.791582\n",
      "[Epoch 170] train=0.999659 val=0.928000 loss=119.256660 time: 56.544277\n",
      "[Epoch 171] train=0.999639 val=0.929300 loss=117.925534 time: 57.349825\n",
      "[Epoch 172] train=0.999479 val=0.928700 loss=135.184317 time: 58.024668\n",
      "[Epoch 173] train=0.999459 val=0.929100 loss=132.082476 time: 58.387326\n",
      "[Epoch 174] train=0.999499 val=0.929100 loss=136.776892 time: 59.024411\n",
      "[Epoch 175] train=0.999379 val=0.928600 loss=141.294378 time: 60.454900\n",
      "[Epoch 176] train=0.999439 val=0.929300 loss=140.062050 time: 61.575210\n",
      "[Epoch 177] train=0.999459 val=0.929100 loss=122.173066 time: 58.296562\n",
      "[Epoch 178] train=0.999579 val=0.929700 loss=121.681833 time: 61.306516\n",
      "[Epoch 179] train=0.999740 val=0.929200 loss=117.205628 time: 59.628387\n",
      "[Epoch 180] train=0.999539 val=0.930000 loss=125.778736 time: 58.184309\n",
      "[Epoch 181] train=0.999539 val=0.929700 loss=121.059334 time: 60.306754\n",
      "[Epoch 182] train=0.999439 val=0.930500 loss=118.593583 time: 59.318484\n",
      "[Epoch 183] train=0.999599 val=0.929700 loss=115.163620 time: 58.505073\n",
      "[Epoch 184] train=0.999599 val=0.929400 loss=114.085539 time: 57.993865\n",
      "[Epoch 185] train=0.999579 val=0.930300 loss=117.119246 time: 58.182313\n",
      "[Epoch 186] train=0.999459 val=0.930700 loss=127.130518 time: 58.856752\n",
      "[Epoch 187] train=0.999479 val=0.930200 loss=118.435439 time: 56.212695\n",
      "[Epoch 188] train=0.999639 val=0.930300 loss=102.674143 time: 59.312086\n",
      "[Epoch 189] train=0.999599 val=0.929700 loss=116.207324 time: 56.970000\n",
      "[Epoch 190] train=0.999619 val=0.929900 loss=111.622713 time: 57.310998\n",
      "[Epoch 191] train=0.999639 val=0.930700 loss=109.776200 time: 57.306001\n",
      "[Epoch 192] train=0.999619 val=0.930000 loss=110.466996 time: 56.834252\n",
      "[Epoch 193] train=0.999619 val=0.929300 loss=116.275382 time: 57.048563\n",
      "[Epoch 194] train=0.999659 val=0.929800 loss=107.609272 time: 57.601429\n",
      "[Epoch 195] train=0.999619 val=0.930000 loss=104.065382 time: 58.211999\n",
      "[Epoch 196] train=0.999579 val=0.930000 loss=113.656221 time: 64.304624\n",
      "[Epoch 197] train=0.999499 val=0.929600 loss=113.841330 time: 60.258893\n",
      "[Epoch 198] train=0.999539 val=0.929300 loss=111.123541 time: 58.050522\n",
      "[Epoch 199] train=0.999619 val=0.929700 loss=110.968454 time: 57.042510\n",
      "[Epoch 200] train=0.999559 val=0.929500 loss=109.417152 time: 56.901999\n",
      "[Epoch 201] train=0.999740 val=0.929500 loss=93.352990 time: 56.764997\n",
      "[Epoch 202] train=0.999619 val=0.929600 loss=103.847159 time: 57.298002\n",
      "[Epoch 203] train=0.999740 val=0.930100 loss=94.327508 time: 56.954517\n",
      "[Epoch 204] train=0.999659 val=0.928700 loss=103.631476 time: 56.501051\n",
      "[Epoch 205] train=0.999700 val=0.930300 loss=99.358247 time: 56.773997\n",
      "[Epoch 206] train=0.999740 val=0.930100 loss=106.020074 time: 62.189997\n",
      "[Epoch 207] train=0.999619 val=0.930400 loss=97.446860 time: 59.939999\n",
      "[Epoch 208] train=0.999679 val=0.929400 loss=97.498362 time: 55.982996\n",
      "[Epoch 209] train=0.999780 val=0.930100 loss=92.990627 time: 58.957528\n",
      "[Epoch 210] train=0.999599 val=0.930200 loss=101.107583 time: 56.991518\n",
      "[Epoch 211] train=0.999820 val=0.929600 loss=88.891582 time: 56.031039\n",
      "[Epoch 212] train=0.999679 val=0.929800 loss=99.443075 time: 57.263698\n",
      "[Epoch 213] train=0.999639 val=0.930000 loss=104.157062 time: 57.053523\n",
      "[Epoch 214] train=0.999720 val=0.930300 loss=97.039474 time: 61.601563\n",
      "[Epoch 215] train=0.999860 val=0.930200 loss=79.684524 time: 57.473330\n",
      "[Epoch 216] train=0.999579 val=0.930300 loss=105.063136 time: 56.246323\n",
      "[Epoch 217] train=0.999760 val=0.930000 loss=95.278690 time: 57.127152\n",
      "[Epoch 218] train=0.999639 val=0.930600 loss=100.114450 time: 58.490997\n",
      "[Epoch 219] train=0.999659 val=0.928800 loss=91.677113 time: 56.092430\n",
      "[Epoch 220] train=0.999720 val=0.929600 loss=94.558271 time: 56.703545\n",
      "[Epoch 221] train=0.999760 val=0.930000 loss=88.791517 time: 56.139298\n"
     ]
    }
   ],
   "source": [
    "epochs = 240\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "train_history.plot(save_path='./plose_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained the model for 240 epochs, the plot may look like:\n",
    "\n",
    "|image-aug|\n",
    "\n",
    "We can better observe the process of model training with plots.\n",
    "For example, one may ask what will happen if there's no data augmentation:\n",
    "\n",
    "|image-no-aug|\n",
    "\n",
    "We can see that training error is much lower than validation error.\n",
    "After the model reaches 100\\% accuracy on training data,\n",
    "it stops improving on validation data.\n",
    "These two plots evidently demonstrates the importance of data augmentation.\n",
    "\n",
    "Model Saving and Loading\n",
    "------------------------\n",
    "\n",
    "After training, we usually want to save it for later use.\n",
    "This is simply done with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('./cifar10_utilization_ResNet/cifar10_utilization_resnet56_v1.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time if you need to use it, just run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_parameters('./cifar10_utilization_ResNet/cifar10_utilization_resnet56_v1.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.plot(save_path='./cifar10_utilization_ResNet/cifar10_utilization_resnet56_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step\n",
    "---------\n",
    "\n",
    "This is the end of our adventure with ``CIFAR10``, but there are many\n",
    "more datasets and algorithms in computer vision!\n",
    "\n",
    "If you would like to know how to train a model on a much larger dataset\n",
    "than ``CIFAR10``, e.g. ImageNet, please read `ImageNet Training <dive_deep_imagenet.html>`__.\n",
    "\n",
    "Or, if you want like to know what can be done with the model you just\n",
    "trained, please read the tutorial on `Transfer learning <transfer_learning_minc.html>`__.\n",
    "\n",
    ".. |image-no-aug| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/overfitting.png\n",
    ".. |image-aug| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/normal_training.png\n",
    ".. |image-golden-bridge| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/golden-bridge.png\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
